---
title: "Predicting High-Income Individuals: A Data-Driven Analysis of the UCI Adult Dataset"
author: "Team idk"
date: "2025-10-22"
output:
  html_document:
    toc: true
    toc_depth: 3
    theme: readable
---

```{r setup, include=FALSE}
options(repos = c(CRAN = "https://cloud.r-project.org"))


install_if_missing <- function(pkg) {
  if (!requireNamespace(pkg, quietly = TRUE)) {
    install.packages(pkg, repos = "https://cloud.r-project.org")
  }
  library(pkg, character.only = TRUE)
}

install_if_missing("caret")
install_if_missing("tidyverse")
install_if_missing("rcompanion")
install_if_missing("fastDummies")
install_if_missing("randomForest")

```

# PROJECT 2: UCI Adult - Analyzing Income Factors

**Goal**: Perform a data analysis project on the UCI Adult Census database and, at the end, build a prediction using machine learning to predict the income variable based on the other labels in the database.

## Step 1: Data cleaning and preparation

### 1. Importing the data

Load the dataset and convert "?" into NA to properly mark missing values.

```{r, include=FALSE, warning=FALSE}
library(dplyr)

```

```{r, echo=FALSE, warning=FALSE}

data <- read.csv("adult.csv",sep=',', na.strings = c("?"," ?", "? "))

head(data,5)


```

Here we can see the table format, on which we based the project.

### 2. Workclass and occupation columns: Handling the missing values

Check the distribution of workclass and occupation, to understand how we will handle the missing values ​​from these classes.

```{r, include=FALSE, warning=FALSE}
data_sum_wc <- data %>%
  filter(!is.na(workclass)) %>%
  count(workclass, sort=TRUE)

data_sum_occ <- data %>%
  filter(!is.na(occupation)) %>%
  count(occupation, sort=TRUE)

data_sum_wc
data_sum_occ
```

Both classes have a good distribution of their values. Except for the "private" value of the Workclass column, whose value exceeds the value of the other classes by a factor of 10.

Identifying how many missing values exist overall and by column using the function is.na and colSums to count the missing values for each column.

```{r, include=FALSE, warning=FALSE}

stack(sort(colSums(is.na(data)), decreasing = TRUE))


```

We find 1843 missing values for the column occupation, 1836 missing values for the column workclass and 583 missing values for the column native.country.

Now we're going to deal with these missing values ​​one by one.

Since people over 60 and under 18 usually do not have an occupation, we count how many of them are affected.

```{r, echo=FALSE,warning=FALSE}

na_over_60_work = sum(is.na(data$workclass) & data$age >= 60)

na_under_18_work = sum(is.na(data$workclass) & data$age <= 18)

na_over_60_occ = sum(is.na(data$occupation) & data$age >= 60)

na_under_18_occ = sum(is.na(data$occupation) & data$age <= 18)

data.frame(
  Variable = c("Workclass (>=60)", "Workclass (<=18)", "Occupation (>=60)", "Occupation (<=18)"),
  Missing_Count = c(na_over_60_work, na_under_18_work, na_over_60_occ, na_under_18_occ)
)


```

There is a difference of four individuals under 18, so we check their data.

```{r, include=FALSE}
filter(data, is.na(data$occupation), data$age <= 18, !is.na(data$workclass))
# the workclass is never-worked for these guys
```

And we realize that the four individuals that have workclass but not occupation, have the label "Never-worked" on the column workclass.

So far we can classify all people under 18 as students who have never worked. Therefore, for all individuals under 18, the workclass is set to "Never-worked" and the occupation is set to "Student".

```{r, include=FALSE}
#student
data <- data %>%
  mutate(
    workclass  = if_else(age <= 18 & is.na(workclass), "Never-worked", workclass),
    occupation = if_else(age <= 18 & is.na(occupation), "Student", occupation)
  )
```

Also, all people over 60 years old do not have workclass and occupation information, so we can consider them all as retired.

However, according to research, the average retirement age is around 65 for men and 63 for women; therefore, we set the workclass and occupation of individuals at or above these ages to "Retired."

```{r, include=FALSE}

data$workclass <- ifelse(
  (data$sex == "Male" & data$age >= 65 & is.na(data$workclass)) |
  (data$sex == "Female" & data$age >= 63 & is.na(data$workclass)),
  "Retired", data$workclass
)
data$occupation <- ifelse(
  (data$sex == "Male" & data$age >= 65 & is.na(data$occupation)) |
  (data$sex == "Female" & data$age >= 63 & is.na(data$occupation)),
  "Retired", data$occupation
)

```

Now, to deal with the other empty Workclass and occupation cases, we'll do a more in-depth analysis.

First, check whether the other vairables are related to a person's occupation or workclass using Chi-square and Cramér’s V.

Chi-Square Test is a statistical test used to check if there is a relationship between two categorical variables.

Cramér’s V is a measure of the strength of the association between two categorical variables. Even if the chi-square test indicates that an association exists, it doesn't show how strong that relationship is, so we use Cramer's V to verify it.

```{r, echo=FALSE, warning=FALSE}
#install.packages("rcompanion")
library(rcompanion)


chi_square_summary <- function(data, target) {
  results <- data.frame(var = character(), p_value = numeric(), cramerV = numeric(), stringsAsFactors = FALSE)
  
  for (col in names(data)) {
    if (col != target && (is.factor(data[[col]]) || is.character(data[[col]]))) {
      test <- tryCatch({
        tbl <- table(data[[target]], data[[col]])
        list(p = chisq.test(tbl)$p.value,
             cv = cramerV(tbl))
      }, error = function(e) NULL)
      
      if (!is.null(test)) {
        results <- rbind(results, data.frame(var = col, p_value = test$p, cramerV = test$cv))
      }
    }
  }
  results[order(results$cramerV, decreasing = TRUE), ]
}
chi_square_summary(data, "workclass")
chi_square_summary(data, "occupation")
```

No other categorical variables show a significant relationship with workclass. Since most entries in this column are labeled "Private", the missing values are replaced with this category for consistency.

```{r, include=FALSE}
data$workclass[is.na(data$workclass)] <- "Private"
```

We found that only the variables sex (small p_value and 0.43 of Cramér’s V) and possibly income (small p_value and 0.35 of Cramér’s V) are relevant for distinguishing or structuring the remaining values in occupation. Therefore, we use these two variables to fill in the missing values for occupation. The code automatically fill the missing values ​​for the occupation variable based on the distribution of occupations within each gender and income group, using the global distribution as a fallback when the group has no data.

```{r, include=FALSE}

set.seed(42)
# global table for fallback
freq_global <- table(data$occupation, useNA = "no")
probs_global <- freq_global / sum(freq_global)
levels_global <- names(probs_global)

data <- data %>%
  group_by(sex, income) %>%
  group_modify(~{
    df <- .x
    idx_na <- which(is.na(df$occupation))
    if (length(idx_na) > 0) {
      tbl <- table(df$occupation, useNA = "no")
      if (sum(tbl) > 0) {
        probs <- tbl / sum(tbl)
        choices <- names(probs)
        probs_vec <- as.numeric(probs)
      } else {
        # fallback global
        choices <- levels_global
        probs_vec <- as.numeric(probs_global)
      }
      df$occupation[idx_na] <- sample(choices, size = length(idx_na),
                                      replace = TRUE, prob = probs_vec)
    }
    return(df)
  }) %>%
  ungroup()


```

We check the distribution of occupation again to see if the values are too concentrated in one category.

```{r, include=FALSE}
data_sum_occ2 <- data %>%
  filter(!is.na(occupation)) %>%
  count(occupation, sort=TRUE)

data_sum_occ2
```

and check if there is any NA value is in workclass and occupation.

```{r, include=FALSE}
sum(is.na(data$workclass))
sum(is.na(data$occupation))

# view rows where the workclass value is NA
data %>%
  filter(is.na(workclass))

# view rows where the occupation value is NA
data %>%
  filter(is.na(occupation))
```

This confirmed that the occupancy values ​​maintained their distributions and that no more values ​​were missing from these columns.

### 3. Checking the redundancy of the education and education-num columns

Create a relationship table between education and education.num, and check whether each pair has a one-to-one mapping.

```{r, include=FALSE}

edu_relation <- data %>%
  filter(!is.na(education), !is.na(education.num)) %>%
  distinct(education, education.num) %>%
  arrange(education.num, education)

edu_relation

edu_to_num <- edu_relation %>%
  count(education) %>%
  filter(n > 1)

num_to_edu <- edu_relation %>%
  count(education.num) %>%
  filter(n > 1)

one_to_one <- nrow(edu_to_num) == 0 & nrow(num_to_edu) == 0

one_to_one
```

Since the two variables represent the same information, we keep education.num, which is more suitable for modeling, and remove the education column.

```{r, include=FALSE}

data <- select(data, -education)

```

### 4. Categorical variables

Remove leading and trailing whitespaces from all character-type categorical variables using the function mutate_if and trimws.

```{r, include=FALSE}
data <- data %>% mutate_if(is.character, trimws)
data <- data %>% mutate_if(is.factor, trimws)
```

## Step 2: Feature Transformation and Engineering

### 1. Grouping Categories

The native-coutry variable has many unique categories. We group all countries outside of the "united - states" into a single cathegory called "Other", including the missing values, to simplify de model.

Checking how many categories are in native-country variable and frequency.

```{r, include=FALSE}
unique(data$native.country)

table_native_country <- count(data, native.country, name = "frequency") %>%
  arrange(desc(frequency))

table_native_country
```

The native-coutry variable has many unique categories. But, how the United-Sates has the majority (29170 cases), we group all countries outside of the "united - states" into a single cathegory called "Other" to simplify de model.

```{r, echo= FALSE,warning=FALSE}

data$native.country[is.na(data$native.country) | data$native.country != "United-States"] <- "Other"

data$native.country <- factor(data$native.country)

table(data$native.country)

```

Here we see the difference in quantity between the two variables.

### 2. Target Transformation

The income variable is text-based, so we convert it into a binary variable (0 for \<= 50k and 1 for \>50k)

Convert income variables into 0 or 1 and then changing the type to numeric.

```{r, include=FALSE}
unique(data$income)

data$income <- ifelse(data$income == ">50K", 1, 0)

data$income <- as.numeric(data$income)

table(data$income)

```

### 3. Age Discretization

Transforming the continuous age variable into a categorical "AgeGroup" (Young, Adult and Senior) to see if specific age groups have different income levels

First, check the summary statistics of the age variable.

```{r, echo=FALSE, warning=FALSE}
summary(data$age)

```

Based on the summary statistics, we defined age groups as follows:

-   Values up to the 1st quartile (28 years) as Young

-   Between the 1st and 3rd quartiles as Adult

-   Above the 3rd quartile (48 years) as Senior.

```{r, include=FALSE}
data <- data %>%
  mutate(
    AgeGroup = case_when(
      is.na(age) ~ NA_character_,
      age <= 28 ~ "Young",        # till Q1
      age <= 48 ~ "Adult",        # between Q1 and Q3
      TRUE ~ "Senior"             # above the Q3
    ),
    AgeGroup = factor(AgeGroup, levels = c("Young", "Adult", "Senior"))
  )

table(data$AgeGroup)

```

Check if specific age groups have different income levels.

```{r, echo=FALSE, warning=FALSE}
prop.table(table(data$AgeGroup, data$income), margin = 1)
table(data$AgeGroup, data$income)
```

### 4. Analyzing Capital Gains/Losses

Create a new column "net_capital" variable by subtracting the columns capital.gain and capital.loss.

```{r, include=FALSE}
data <- data %>%
  mutate(net_capital = capital.gain - capital.loss)
data[,"net_capital"]
colnames(data)
```

Analyzing net_capital is a good predictor of income by using Spearman's correlation.

Spearman's correlation measures the monotonic association between two variables that is, whether one tends to increase when the other increases, without assuming linearity.

```{r, include=FALSE}
# correlação
cor.test(data$net_capital, as.numeric(data$income), method = "spearman")

```

The Spearman correlation between net_capital and income is 0.137, indicating a weak but statistically significant positive relationship. Individuals with higher net capital tend to have slightly higher income levels.

To provide a better quantitative overview, the table below shows the quantity, by "AgeGroup" and "income," of the number of people per "net_capital." Positive values ​​indicate positive "net_capital," and so on.

```{r, echo=FALSE, warning=FALSE}
summary_capital <- data %>%
  group_by(AgeGroup, income) %>%
  summarise(
    positive = sum(net_capital > 0, na.rm = TRUE),
    zero     = sum(net_capital == 0, na.rm = TRUE),
    negative = sum(net_capital < 0, na.rm = TRUE),
    total    = n(),
    .groups = "drop"
  )

summary_capital
```

## Step 3: Visualization and Exploration

### 1. Income and Education Level

Visualizing the proportion of people earning over 50k for each of "education-num" using a bar diagram.

```{r, include=FALSE}
library(dplyr)
library(ggplot2)
library(scales)
```

```{r, echo=FALSE,warning=FALSE}

edu_income <- data %>%
  group_by(education.num) %>%
  summarise(
    n = n(),
    n_high = sum(income == 1),
    prop_high = n_high / n
  ) %>%
  arrange(education.num)

ggplot(edu_income, aes(x = factor(education.num), y = prop_high)) +
  geom_col() +
  geom_text(aes(label = percent(prop_high, accuracy = 0.1)), vjust = -0.5, size = 3) +
  scale_y_continuous(labels = percent_format(accuracy = 1), limits = c(0,1), expand = expansion(c(0,0.05))) +
  labs(x = "education.num", y = "Proportion >50K", title = "Proportion of >50K for education-num") +
  theme_minimal()


```

**What it shows**: Proportion of individuals earning \> \$50K by education.num.

**Conclusion**: Higher education levels are associated with a higher share of high-income individuals.

### 2. Work Hours and Income

Comparing the distribution of hours-per-week between the two income classes

Using geom_violin()

```{r, echo=FALSE, warning=FALSE}
ggplot(data, aes(x = factor(income), y = hours.per.week)) +
  geom_violin(trim = TRUE) +
  stat_summary(fun = median, geom = "point", size = 2, color = "black") +
  coord_flip() +
  labs(title = "Violin: hours-per-week por income", x = "Income", y = "Hours per week") +
  theme_minimal()

```

**What it shows**: Distribution of hours.per.week across income classes.

**Conclusion**: The \>\$50K group generally works longer hours per week.

### 3. Impact of Marital Status

Visualizing the relationship between marital-status and income.

```{r, echo=FALSE, warning=FALSE}
ggplot(data, aes(x = marital.status, fill = factor(income))) +
  geom_bar(position = "fill") +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  coord_flip() +
  labs(
    x = "Civil State",
    y = "Proportion within the group",
    fill = "Income (0 = ≤50K, 1 = >50K)",
    title = "Impact of marital status on income"
  ) +
  theme_minimal()
```

**What it shows**: Within each marital.status, the proportion of ≤/\> \$50K.

**Conclusion:** Married individuals (esp. with spouse present) show higher high-income shares.

### 4. Age Distributions

Visualizing the age variable, facted by the income class, to observe distributional differences.

```{r, echo=FALSE, warning=FALSE}
ggplot(data, aes(x = age)) +
  geom_histogram(bins = 30, alpha = 0.8) +
  facet_wrap(~ income) +
  labs(title = "Age distribution by income class", x = "Age", y = "Count") +
  theme_minimal()
```

**What it shows**: age histograms faceted by income class.

**Conclusion:** High-income individuals concentrate more in middle ages.

### 5. Add 2 visualizations of your liking.

**1. Age vs Work Hours: explore the relationship between age and hours-per-week by income.**

```{r, echo=FALSE, warning=FALSE}
ggplot(data, aes(x = AgeGroup, y = hours.per.week, fill = factor(income))) +
  geom_boxplot(alpha = 0.7, outlier.shape = NA) +
  scale_fill_manual(values = c("#6BAED6", "#FD8D3C"),
                    name = "Income",
                    labels = c("≤50K", ">50K")) +
  labs(
    title = "Work Hours Distribution by Age Group and Income",
    x = "Age Group",
    y = "Hours per Week"
  ) +
  theme_minimal() +
  theme(legend.position = "top")
```

**What it shows**: hours.per.week distribution by AgeGroup and income.

**Conclusion**: Seniors work fewer hours; \>\$50K consistently shows higher hours across groups.

**2. Income Distribution (\>\$50k) by Gender and Education Level** \*

```{r, echo=FALSE, warning=FALSE}

edu_sex_income <- data %>%
  group_by(sex, education.num) %>%
  summarise(
    total = n(),
    high_income = sum(income == 1),
    prop_high = high_income / total,
    .groups = "drop"
  )

# ggplot(edu_sex_income, aes(x = factor(education.num), y = prop_high, fill = sex)) +
#   geom_col(position = "dodge") +
#   scale_y_continuous(labels = percent_format(accuracy = 1)) +
#   labs(
#     title = "Proportion of people with income >50K by gender and education level.",
#     x = "Education Level (education.num)",
#     y = "Proportion with income >50K",
#     fill = "Sex"
#   ) +
#   theme_minimal()

ggplot(edu_sex_income, aes(x = education.num, y = prop_high, color = sex)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  labs(
    title = "Income >50K by sex and education level",
    x = "Education Level",
    y = "Proportion with income >50K",
    color = "Sex"
  ) +
  theme_minimal()

```

**What it shows**: Proportion of \>\$50K by gender over education.num.

**Conclusion**: Both genders benefit from education; males remain higher across levels.

same grafic that the above. Just a diferent whay to see it.

**3. Income Distribution (\>50k) across Races and Education Levels.**

```{r, echo=FALSE, warning=FALSE}
edu_race_income <- data %>%
  group_by(race, education.num) %>%
  summarise(
    total = n(),
    high_income = sum(income == 1),
    prop_high = high_income / total,
    .groups = "drop"
  )

ggplot(edu_race_income, aes(x = education.num, y = prop_high, color = race)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  labs(
    title = "Income >50K by race and education level",
    x = "Education Level",
    y = "Proportion with income >50K",
    color = "Race"
  ) +
  theme_minimal()

```

**What it shows**: Proportion of \>\$50K across races over education.num.

**Conclusion**: Education lifts all races, but racial disparities remain visible.

## Step 4: Modeling and Prediction

### 1. Preparing Data for Modeling

Converting all remaining categorical variables into "dummy" variables, using one-hot encoding, to use these variables in the machine learning models.

```{r, include=FALSE}
#install.packages("fastDummies")
library(fastDummies)
library(dplyr)
```

```{r, include=FALSE}

data_chars <- data %>% select(where(is.character))
cat_cols = names(data_chars)
```

The first dummy column is removed to prevent multicollinearity.

```{r, include=FALSE}
#create dummies variables
data_dummies <- fastDummies::dummy_cols(
  data,
  select_columns = cat_cols,
  remove_first_dummy = TRUE, #evita multicolinearidade
  remove_selected_columns = TRUE #remove the original columns
)
```

Check the new data structure to see with works.

```{r, include=FALSE}
str(data_dummies)
sum(is.na(data_dummies))
```

Now, we can start the preparation of the modeling.

```{r, include=FALSE}
#install.packages("caret")
```

### 2. Logistic Regression

Training a logistic regression model to predict the income class, for understand the influence of each variable.

Split the Dataset into Training and Test Sets.

```{r, include=FALSE}
#logistic regression
set.seed(42)

n_rows <- nrow(data_dummies)
train_index <- sample(1:n_rows, 0.8*n_rows)

train <- data_dummies[train_index,]
test <- data_dummies[-train_index,]

#train <- na.omit(train)
#test <- na.omit(test)

```

Building the Logistic Regression Model, training a logistic regression model using all available predictors to estimate the probability of earning more than 50K.

```{r, include=FALSE}
#training the model
#glm - generalized linear model
train$income <- as.factor(train$income)
model_log <- glm(income~ ., data=train,family = binomial(link = "logit"))
summary(model_log)
```

Make predictions and Classify Outcomes.

```{r, include=FALSE}
pred_prob <- predict(model_log,newdata = test,type="response")
pred_class <- ifelse(pred_prob>0.5,1,0) #prob to class 1 or 0
```

Evaluate Logistic Regression Accuracy. Generate a confusion matrix and compute model accuracy to assess predictive performance of logistic regression.

```{r, echo=FALSE, warning=FALSE}
# Confusion matrix
table(Predict = pred_class, Real = test$income)

# Accuracy
accuracy_log <- mean(pred_class == test$income)
accuracy_log

# to ensure that both are factors with equal levels
truth <- factor(as.character(test$income), levels = c("0","1"))
pred  <- factor(as.character(pred_class),  levels = c("0","1"))

precision <- posPredValue(pred, truth, positive = "1")
recall    <- sensitivity(pred, truth, positive = "1")

precision
recall
```

For the logistic model, we have:

-   Accuracy = 0.85

-   Precision = 0.73

-   Recall = 0.60

### 3. Random Forest

Install and loading Random Forest.

```{r, include=FALSE}
#install.packages("randomForest")
```

Build a Random Forest model using 500 trees to predict income class, and evaluate variable importance during model training.

```{r, include=FALSE}
#random Forest
library(randomForest)
library(pROC)

set.seed(42)

#correct names
names(train) <- make.names(names(train))
names(test)  <- make.names(names(test))

model_rf <- randomForest(
  income ~ .,
  data = train,
  ntree = 500, #number of trees
  mtry = (sqrt(ncol(train)-1)), #number of variables tested by division
  importance=TRUE
)
model_rf
```

Predict income on the test dataset using the Random Forest model and calculate accuracy from the confusion matrix.

```{r, echo=FALSE, warning=FALSE}
pred_rf_class <- predict(model_rf, newdata = test)
pred_rf_prob <- predict(model_rf, newdata = test, type='prob')[,2]

conf_rf <- table(Predict = pred_rf_class, Real = test$income)

accuracy_rf <- mean(pred_rf_class == test$income)
accuracy_rf
conf_rf

truth <- factor(as.character(test$income), levels = c("0","1"))
pred  <- factor(as.character(pred_rf_class),  levels = c("0","1"))

precision <- posPredValue(pred, truth, positive = "1")
recall    <- sensitivity(pred, truth, positive = "1")

precision
recall

# varImpPlot(model_rf, main = "Importance of variables")
```

For the Random Forest model, we have:

-   Accuracy = 0.87

-   Precision = 0.76

-   Recall = 0.62

Compare the accuracy of logistic regression and random forest to identify which model performs better.

```{r, echo=FALSE, warning=FALSE}
cat("Accuracy Logistic:", round(accuracy_log, 3), "\n")
cat("Accuracy Random Forest: ", round(accuracy_rf,3), "\n")
```

Plot the ROC curve and calculate the AUC (Area Under Curve) for logistic regression to evaluate how well the model separates income classes.

```{r, include=FALSE}
# For RL
confusionMatrix(as.factor(pred_class), as.factor(test$income), positive = "1")
```

```{r, echo=FALSE, warning=FALSE}

# ROC and AUC
roc_log <- roc(test$income, pred_prob)
plot(roc_log, col = "blue", main = "ROC Curve - Logistic Regression")
auc_log <- auc(roc_log)
auc_log
```

The area under the ROC curve for the logistical model was 0.909.

Evaluating Random Forest: Assess the Random Forest model with confusion matrix, ROC curve, and AUC score to measure predictive performance.

```{r, include=FALSE}
confusionMatrix(pred_rf_class, as.factor(test$income), positive="1")
```

```{r, echo=FALSE, warning=FALSE}
#confusion matrix and AUC for RF
library(caret)
library(pROC)

roc_rf <- roc(as.numeric(test$income), as.numeric(pred_rf_prob))
plot(roc_rf, col = "darkgreen", main = "ROC Curve - Random Forest")
auc(roc_rf)
```

The area under the ROC curve for the logistical model was 0.910.

Display Predicted Class Distribution

```{r, echo=FALSE, warning=FALSE}
table(pred_rf_class)
```

Identify the Most Important Variables.

```{r, echo=FALSE, warning=FALSE}

importance_df <- as.data.frame(importance(model_rf))
top5_vars <- importance_df %>%
  arrange(desc(MeanDecreaseGini)) %>%
  head(5)
top5_vars
```
